{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Mixtures of Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how mixtures of experts can be used to boost performance.\n",
    "\n",
    "The objective of this lab is to classify images from Cifar10 (https://www.cs.toronto.edu/~kriz/cifar.html) to one of ten classes: {0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer, 5: dog, 6: frog, 7: horse, 8: ship, 9: truck}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cifar10](cifar10_resize.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, a gating function is trained to pass examples to two experts which are trained separatly, where one expert is trained to classify images within the \"natural image\" category (e.g. cat, dog, etc) and another to classify images within the  \"artificial image\" category (e.g. plane, car). The experts are then used to boost the performance of a baseline architecture that classifies image to one of the 10 classes.\n",
    "\n",
    "Specifically, the mixture is built in the following order:\n",
    "1. A single model is trained to to classify all 10 classes. (This is included in the mixture, and is also our evaluation benchmark)\n",
    "\n",
    "2. An expert gating function is trained to recognise whether an image is of an artificial or  natural subject.\n",
    "\n",
    "3. An artificial expert is trained to classify artificial objects that have a label in {0, 1, 8, 9}.\n",
    "\n",
    "4. A natural expert is trained to classify natural objects that have a label in  {2, 3, 4, 5, 6, 7}.\n",
    "\n",
    "5. A gating function is trained to determine the contribution of the experts and the contribution of the baseline architecture to the final output.\n",
    "\n",
    "6. The mixture is built as illustrated in the figure below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](moe_architecture_illus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import concatenate, Lambda, Reshape\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import pydot\n",
    "from IPython.display import SVG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (not to be changed)\n",
    "orig_classes = 10\n",
    "gate0_classes = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Parameters\n",
    "\n",
    "You can try changing the mixture parameters in the following piece of code, when doing so, consider the following:\n",
    "\n",
    "1 - Increasing the number of epochs increases the fit to training data, at some point this should cause over-fitting. Conversly, setting it low should cause under-fitting.\n",
    "\n",
    "2 - Increasing the number of training examples increases the number of learnable features.\n",
    "\n",
    "3 - Using a large model for different classifiers increases their capacity to learn. This increases the amount of required epochs for training, and also increases the risk of over-fitting.\n",
    "\n",
    "By changing the parameters, the performance of the mixture of experts and the baseline classifier should change accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training/testing examples per batch\n",
    "batch_size = 50\n",
    "\n",
    "# Training epochs. A higher number of epochs corresponds to \"more fitting to training data\"\n",
    "epochs = 1\n",
    "\n",
    "# Number of training/testing examples to use\n",
    "train_examples = 5000  # Max is 50000\n",
    "test_examples = 1000  # Max is 5000\n",
    "\n",
    "# Large/small model flags. Set to true to change a classifier to \"large\"\n",
    "use_large_experts = False\n",
    "use_large_gating_mlp = False\n",
    "use_large_baseline_classifier = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete previous model checkpoints\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"gate0Cifar10\", ignore_errors=True)\n",
    "shutil.rmtree(\"moe3Cifar10\", ignore_errors=True)\n",
    "shutil.rmtree(\"natureCifar10\", ignore_errors=True)\n",
    "shutil.rmtree(\"baseCifar10\", ignore_errors=True)\n",
    "shutil.rmtree(\"artCifar10\", ignore_errors=True)\n",
    "\n",
    "\n",
    "# get the newest model file within a directory\n",
    "def getNewestModel(model, dirname):\n",
    "    from glob import glob\n",
    "\n",
    "    target = os.path.join(dirname, \"*\")\n",
    "    files = [(f, os.path.getmtime(f)) for f in glob(target)]\n",
    "    if len(files) == 0:\n",
    "        return model\n",
    "    else:\n",
    "        newestModel = sorted(files, key=lambda files: files[1])[-1]\n",
    "        model.load_weights(newestModel[0])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset  ; X: input images,  Y: class label ground truth\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train[:train_examples]\n",
    "x_test = x_test[:test_examples]\n",
    "y_train = y_train[:train_examples]\n",
    "y_test = y_test[:test_examples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare x dataset\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train /= 255\n",
    "x_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "y_train0 = keras.utils.to_categorical(y_train, orig_classes)\n",
    "y_test0 = keras.utils.to_categorical(y_test, orig_classes)\n",
    "\n",
    "print(\"y train0:{0}\\ny test0:{1}\".format(y_train0.shape, y_test0.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "cifarInput = Input(shape=(x_train.shape[1:]), name=\"input\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small VGG-like model\n",
    "def simpleVGG(cifarInput, num_classes, name=\"vgg\"):\n",
    "    name = [name + str(i) for i in range(12)]\n",
    "\n",
    "    # convolution and max pooling layers\n",
    "    vgg = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", name=name[0])(\n",
    "        cifarInput\n",
    "    )\n",
    "    vgg = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", name=name[1])(vgg)\n",
    "    vgg = MaxPooling2D(pool_size=(2, 2), name=name[2])(vgg)\n",
    "    vgg = Dropout(0.25, name=name[3])(vgg)\n",
    "    vgg = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", name=name[4])(vgg)\n",
    "    vgg = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", name=name[5])(vgg)\n",
    "    vgg = MaxPooling2D(pool_size=(2, 2), name=name[6])(vgg)\n",
    "    vgg = Dropout(0.25, name=name[7])(vgg)\n",
    "\n",
    "    # classification layers\n",
    "    vgg = Flatten(name=name[8])(vgg)\n",
    "    vgg = Dense(512, activation=\"relu\", name=name[9])(vgg)\n",
    "    vgg = Dropout(0.5, name=name[10])(vgg)\n",
    "    vgg = Dense(num_classes, activation=\"softmax\", name=name[11])(vgg)\n",
    "    return vgg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large VGG-like model\n",
    "def fatVGG(cifarInput, num_classes, name=\"vgg\"):\n",
    "    name = [name + str(i) for i in range(17)]\n",
    "\n",
    "    # convolution and max pooling layers\n",
    "    vgg = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", name=name[0])(\n",
    "        cifarInput\n",
    "    )\n",
    "    vgg = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", name=name[1])(vgg)\n",
    "    vgg = MaxPooling2D(pool_size=(2, 2), name=name[2])(vgg)\n",
    "    vgg = Dropout(0.25, name=name[3])(vgg)\n",
    "    vgg = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", name=name[4])(vgg)\n",
    "    vgg = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", name=name[5])(vgg)\n",
    "    vgg = MaxPooling2D(pool_size=(2, 2), name=name[6])(vgg)\n",
    "    vgg = Dropout(0.25, name=name[7])(vgg)\n",
    "    vgg = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", name=name[8])(vgg)\n",
    "    vgg = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", name=name[9])(vgg)\n",
    "    vgg = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", name=name[10])(vgg)\n",
    "    vgg = MaxPooling2D(pool_size=(2, 2), name=name[11])(vgg)\n",
    "    vgg = Dropout(0.25, name=name[12])(vgg)\n",
    "\n",
    "    # classification layers\n",
    "    vgg = Flatten(name=name[13])(vgg)\n",
    "    vgg = Dense(512, activation=\"relu\", name=name[14])(vgg)\n",
    "    vgg = Dropout(0.5, name=name[15])(vgg)\n",
    "    vgg = Dense(num_classes, activation=\"softmax\", name=name[16])(vgg)\n",
    "    return vgg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first gating network, to decide artificial or natural object\n",
    "if use_large_gating_mlp:\n",
    "    gate0VGG = fatVGG(cifarInput, gate0_classes, \"gate0\")\n",
    "else:\n",
    "    gate0VGG = simpleVGG(cifarInput, gate0_classes, \"gate0\")\n",
    "gate0Model = Model(cifarInput, gate0VGG)\n",
    "\n",
    "# base VGG\n",
    "if use_large_baseline_classifier:\n",
    "    baseVGG = fatVGG(cifarInput, orig_classes, \"base\")\n",
    "else:\n",
    "    baseVGG = simpleVGG(cifarInput, orig_classes, \"base\")\n",
    "baseModel = Model(cifarInput, baseVGG)\n",
    "\n",
    "# artificial expert VGG\n",
    "if use_large_experts:\n",
    "    artificialVGG = fatVGG(cifarInput, orig_classes, \"artificial\")\n",
    "else:\n",
    "    artificialVGG = simpleVGG(cifarInput, orig_classes, \"artificial\")\n",
    "artificialModel = Model(cifarInput, artificialVGG)\n",
    "\n",
    "# naturalVGG = fatVGG(cifarInput, orig_classes, \"natural\")\n",
    "if use_large_experts:\n",
    "    naturalVGG = fatVGG(cifarInput, orig_classes, \"natural\")\n",
    "else:\n",
    "    naturalVGG = simpleVGG(cifarInput, orig_classes, \"natural\")\n",
    "\n",
    "naturalModel = Model(cifarInput, naturalVGG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 10-Class Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "baseModel.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make saving directory for checkpoints\n",
    "baseSaveDir = \"./baseCifar10/\"\n",
    "if not os.path.isdir(baseSaveDir):\n",
    "    os.makedirs(baseSaveDir)\n",
    "\n",
    "# early stopping and model checkpoint\n",
    "es_cb = EarlyStopping(monitor=\"val_loss\", patience=2, verbose=1, mode=\"auto\")\n",
    "chkpt = os.path.join(baseSaveDir, \"Cifar10_.{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "cp_cb = ModelCheckpoint(\n",
    "    filepath=chkpt, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"auto\"\n",
    ")\n",
    "\n",
    "# load the newest model data from the directory if exists\n",
    "baseModel = getNewestModel(baseModel, baseSaveDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "baseModel.fit(\n",
    "    x_train,\n",
    "    y_train0,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test0),\n",
    "    callbacks=[es_cb, cp_cb],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "baseModel = getNewestModel(baseModel, baseSaveDir)\n",
    "baseScore = baseModel.evaluate(x_test, y_test0)\n",
    "print(baseScore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train 2-Class Natural/Artificial Classifier\n",
    "\n",
    "The expert gating model determines whether an image is \"natural\" or \"artificial\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make ground truth for whether an example is \"natural\" or \"artificial\"\n",
    "y_trainG0 = np.array([0 if i in [0, 1, 8, 9] else 1 for i in y_train])\n",
    "y_testG0 = np.array([0 if i in [0, 1, 8, 9] else 1 for i in y_test])\n",
    "\n",
    "y_trainG0 = keras.utils.to_categorical(y_trainG0, 2)\n",
    "y_testG0 = keras.utils.to_categorical(y_testG0, 2)\n",
    "\n",
    "print(\"y trainG0:{0}\\ny testG0:{1}\".format(y_trainG0.shape, y_testG0.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "gate0Model.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make saving directory for check point\n",
    "gate0SaveDir = \"./gate0Cifar10/\"\n",
    "if not os.path.isdir(gate0SaveDir):\n",
    "    os.makedirs(gate0SaveDir)\n",
    "\n",
    "# early stopping and model checkpoint\n",
    "es_cb = EarlyStopping(monitor=\"val_loss\", patience=2, verbose=1, mode=\"auto\")\n",
    "chkpt = os.path.join(gate0SaveDir, \"Cifar10_.{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "cp_cb = ModelCheckpoint(\n",
    "    filepath=chkpt, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"auto\"\n",
    ")\n",
    "\n",
    "# load the newest model data from the directory if exists\n",
    "gate0Model = getNewestModel(gate0Model, gate0SaveDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "gate0Model.fit(\n",
    "    x_train,\n",
    "    y_trainG0,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_testG0),\n",
    "    callbacks=[es_cb, cp_cb],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "gate0Model = getNewestModel(gate0Model, gate0SaveDir)\n",
    "gate0Score = gate0Model.evaluate(x_test, y_testG0)\n",
    "print(gate0Score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train \"Natural\" and \"Artificial\" Experts\n",
    "<br>\n",
    "The expert networks are specialized in predicting a certain classes.<br>\n",
    "Each network is only trained with its specialized field: the artificial expert get trained for labels 0, 1, 8 and 9; the natural expert for labels 2, 3, 4, 5, 6 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the position of artificial images and natural images in training and test dataset\n",
    "artTrain = [i for i in range(len(y_train)) if y_train[i] in [0, 1, 8, 9]]\n",
    "natureTrain = [i for i in range(len(y_train)) if y_train[i] in [2, 3, 4, 5, 6, 7]]\n",
    "artTest = [i for i in range(len(y_test)) if y_test[i] in [0, 1, 8, 9]]\n",
    "natureTest = [i for i in range(len(y_test)) if y_test[i] in [2, 3, 4, 5, 6, 7]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get artificial dataset and natural dataset\n",
    "x_trainArt = x_train[artTrain]\n",
    "x_testArt = x_test[artTest]\n",
    "y_trainArt = y_train[artTrain]\n",
    "y_testArt = y_test[artTest]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial expert network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for artificial dataset\n",
    "y_trainArt = keras.utils.to_categorical(y_trainArt, orig_classes)\n",
    "y_testArt = keras.utils.to_categorical(y_testArt, orig_classes)\n",
    "\n",
    "print(\"y train art:{0}\\ny test art:{1}\".format(y_trainArt.shape, y_testArt.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "artificialModel.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make saving directory for check point\n",
    "artSaveDir = \"./artCifar10/\"\n",
    "if not os.path.isdir(artSaveDir):\n",
    "    os.makedirs(artSaveDir)\n",
    "\n",
    "# early stopping and model checkpoint\n",
    "es_cb = EarlyStopping(monitor=\"val_loss\", patience=2, verbose=1, mode=\"auto\")\n",
    "chkpt = os.path.join(artSaveDir, \"Cifar10_.{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "cp_cb = ModelCheckpoint(\n",
    "    filepath=chkpt, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"auto\"\n",
    ")\n",
    "\n",
    "# load the newest model data if exists\n",
    "artificialModel = getNewestModel(artificialModel, artSaveDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "artificialModel.fit(\n",
    "    x_trainArt,\n",
    "    y_trainArt,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_testArt, y_testArt),\n",
    "    callbacks=[es_cb, cp_cb],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "artificialModel = getNewestModel(artificialModel, artSaveDir)\n",
    "artScore = artificialModel.evaluate(x_testArt, y_testArt)\n",
    "print(artScore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Natural expert network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for natural dataset\n",
    "x_trainNat = x_train[natureTrain]\n",
    "x_testNat = x_test[natureTest]\n",
    "y_trainNat = y_train[natureTrain]\n",
    "y_testNat = y_test[natureTest]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get natural dataset\n",
    "y_trainNat = keras.utils.to_categorical(y_trainNat, orig_classes)\n",
    "y_testNat = keras.utils.to_categorical(y_testNat, orig_classes)\n",
    "\n",
    "print(\"y train nature:{0}\\ny test nature:{1}\".format(y_trainNat.shape, y_testNat.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "naturalModel.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make saving directory for check point\n",
    "natSaveDir = \"./natureCifar10/\"\n",
    "if not os.path.isdir(natSaveDir):\n",
    "    os.makedirs(natSaveDir)\n",
    "\n",
    "# early stopping and model checkpoint\n",
    "es_cb = EarlyStopping(monitor=\"val_loss\", patience=2, verbose=1, mode=\"auto\")\n",
    "chkpt = os.path.join(natSaveDir, \"Cifar10_.{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "cp_cb = ModelCheckpoint(\n",
    "    filepath=chkpt, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"auto\"\n",
    ")\n",
    "\n",
    "# load the newest model data if exists\n",
    "naturalModel = getNewestModel(naturalModel, natSaveDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "naturalModel.fit(\n",
    "    x_trainNat,\n",
    "    y_trainNat,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_testNat, y_testNat),\n",
    "    callbacks=[es_cb, cp_cb],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "naturalModel = getNewestModel(naturalModel, natSaveDir)\n",
    "natScore = naturalModel.evaluate(x_testNat, y_testNat)\n",
    "print(natScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the weights of all trained models so far (i.e. baseline, experts, and expert gating models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in baseModel.layers:\n",
    "    l.trainable = False\n",
    "for l in gate0Model.layers:\n",
    "    l.trainable = False\n",
    "for l in artificialModel.layers:\n",
    "    l.trainable = False\n",
    "for l in naturalModel.layers:\n",
    "    l.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Connecting the overall networks to form the mixture of experts model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-gate\n",
    "\n",
    "Up to this point, we have a baseline classifier which classifies the 10 classes, we have 2 experts which each classify the 10 classes but specialise in classifying either the natural or artificial categories of classes, and we have a first gate which decides which of the experts to choose for the given cifar input. We want our second gate to be able to take in the cifar input and decide what the importance should be of the output of the i) baseline and ii) chosen expert, thereby determining the importance of the baseline and the chosen expert in producing a final MoE output prediction. To do this, the second gate will be composed of 2 sub-gates; one for each expert.\n",
    "\n",
    "*Sub-gate Structure*: First layer of sub-gate is our (32, 32, 3) input layer as before which takes in the cifar data -> flatten input -> pass through 512 unit dense layer with relu activation function -> pass through dropout layer which randomly sets units to 0 with probability 0.5 -> pass through orig_classes x 2 = 10 x 2 = 20 unit dense layer with softmax activation function -> now have importance values for the i) baseline and ii) chosen expert -> reshape this 1D output into (10, 2) output. I.e. along [:, 0], have a (10,) tensor of what the sub-gate 'thinks' the importance is of the 10 baseline classifier outputs, and along [:,1], have a (10,) tensor of what the sub-gate 'thinks' the importance is of the expert.\n",
    "\n",
    "Instantiate one of these sub-gates for each expert. Whichever expert is chosen by the first binary classifier 'hard' gate will determine which corresponding expert sub-gate is used to 'soft gate' between the baseline and the chosen expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sub-Gate network, for the second gating network layer\n",
    "def subGate(cifarInput, orig_classes, name=\"subGate\"):\n",
    "    name = [name + str(i) for i in range(5)]\n",
    "    subgate = Flatten(name=name[0])(cifarInput)\n",
    "    subgate = Dense(512, activation=\"relu\", name=name[1])(subgate)\n",
    "    subgate = Dropout(0.5, name=name[2])(subgate)\n",
    "    subgate = Dense(orig_classes * 2, activation=\"softmax\", name=name[3])(subgate)\n",
    "    subgate = Reshape((orig_classes, 2), name=name[4])(subgate)\n",
    "    return subgate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the artificial gating network\n",
    "artGate = subGate(cifarInput, orig_classes, \"artExpertGate\")\n",
    "\n",
    "# the natural gating network\n",
    "natureGate = subGate(cifarInput, orig_classes, \"natureExpertGate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-gate Lambda\n",
    "\n",
    "Want this layer to take outputs of i) baseline ii) expert and iii) sub-gate of the corresponding expert -> get logit outputs for each of the 10 orig_classes\n",
    "\n",
    "- Takes in argument gx, which is a list of 3 tensors (the output of i) baseline ii) expert iii) corresponding expert sub-gate)\n",
    "\n",
    "- gx[0] -> baseline output tensor (10,). Is a softmax output for each of the 10 classes\n",
    "\n",
    "- gx[1] -> expert network output tensor (10,). Is a softmax output for each of the 10 classes. Which expert's output reached here depends on the binary classifier output of the first gate (we will implement the logic of choosing the expert when we tie all the models together, see below).\n",
    "\n",
    "- gx[2] -> corresponding expert sub-gate output tensor (10,2,). \n",
    "- gx[2][:,:,0] -> baseline importance tensor of shape (10,). Is what the sub-gate thinks the importance is of each of the 10 baseline output classes. \n",
    "- gx[2][:,:,1] -> expert importance tensor of shape (10,). Is what the sub-gate thinks the importance is of each of the 10 expert output classes.\n",
    "\n",
    "We ultimately want a logit output for each of the 10 classes. We want this output to be determined by what the i) baseline and ii) expert thought, weighted by the importance of what the sub-gate thought. To do this inference, we can define a simple function which: i) multiplies the baseline's output by the sub-gate's baseline importance -> get a (10,) tensor ii) multiplies the expert's output by the sub-gate's expert importance -> get a (10,) tensor iii) sum these two importance-weighted terms to get a final (10,) tensor of logit outputs (one for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inference calculation with Keras Lambda layer with base VGG, expert network and the second gating network of corresponding expert as input\n",
    "# the inference is calculated as sum of multiplications of base VGG inference output and its importance, and expert network inference output and its importance\n",
    "def subGateLambda(base, expert, subgate):\n",
    "    output = Lambda(\n",
    "        lambda gx: (gx[0] * gx[2][:, :, 0]) + (gx[1] * gx[2][:, :, 1]),\n",
    "        output_shape=(orig_classes,),\n",
    "    )([base, expert, subgate])\n",
    "    return output\n",
    "\n",
    "\n",
    "# DEBUG\n",
    "# def subGateLambda(base, expert, subgate):\n",
    "#     output = Lambda(lambda gx: print('\\ngx: {}\\ngx[0]: {}\\ngx[2][:,:,0]: {}\\ngx[1]: {}\\ngx[2][:,:,1]: {}'.format(gx, gx[0],gx[2][:,:,0],gx[1],gx[2][:,:,1])))([base, expert, subgate])\n",
    "#     return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting the Networks\n",
    "\n",
    "Now we just need to i) tie all of the above models together and ii) implement the logic for choosing an expert at the first gate. The first 'expert gate' binary classifier is a 'hard gate' since it will choose one expert and block the other. The second gate (a sub-gate) is a 'soft gate' since it will apply its importance weights across the i) baseline and ii) chosen expert.\n",
    "\n",
    "To do this, we can define a layer which takes the outputs of i) baseline ii) first gate iii) artificial expert iv) natural expert v) artificial sub-gate vi) natural sub-gate.\n",
    "\n",
    "- Takes in argument gx, which is a list of 6 tensors (the output of i) baseline ii) first gate iii) artificial expert iv) natural expert v) artificial sub-gate vi) natural sub-gate)\n",
    "- gx[0] -> baseline output tensor (10,)\n",
    "- gx[1] -> first gate binary output tensor (2,)\n",
    "- gx[2] -> artificial expert output tensor (10,)\n",
    "- gx[3] -> natural expert output tensor (10,)\n",
    "- gx[4] -> artificial sub-gate output tensor (10,2,)\n",
    "- gx[5] -> natural sub-gate output tensor (10,2,)\n",
    "\n",
    "We want to implement the logic that the first gate's chosen expert's output and corresponding sub-gate output should be passed (along with the baseline's output) to the sub-gate lambda function defined previously. To do this, we can use the Keras backend switch function, which will pass the chosen expert's output and its corresponding sub-gate output to the sub-gate lambda depending on which of the 2 outputs of the first gate was greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# connecting the overall networks.\n",
    "# the Keras backend switch works as deciding with the first gating network, leading to artificial or natural gate\n",
    "output = Lambda(\n",
    "    lambda gx: K.switch(\n",
    "        tf.expand_dims(gx[1][:, 0], axis=1) > tf.expand_dims(gx[1][:, 1], axis=1),\n",
    "        subGateLambda(gx[0], gx[2], gx[4]),\n",
    "        subGateLambda(gx[0], gx[3], gx[5]),\n",
    "    ),\n",
    "    output_shape=(orig_classes,),\n",
    ")([baseVGG, gate0VGG, artificialVGG, naturalVGG, artGate, natureGate])\n",
    "\n",
    "# DEBUG\n",
    "# output = Lambda(lambda gx: print('\\ngx: {}\\ngx[0]: {}\\ngx[1]: {}\\ngx[2]: {}\\ngx[3]: {}\\ngx[4]: {}\\ngx[5]: {}'.format(gx, gx[0], gx[1], gx[2], gx[3], gx[4], gx[5])),\n",
    "#                 output_shape=(orig_classes,))([baseVGG, gate0VGG, artificialVGG, naturalVGG, artGate, natureGate])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the mixture of experts model\n",
    "model = Model(cifarInput, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have already trained the baseline, the experts, and the first expert gate. Now we need only train the sub-gates and the final Lambda inference layer, which will take the outputs of the above trained models and learn to output a final (10,) MoE prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show layers and if it's trainable or not\n",
    "# only the second gating network layers and the last Lambda inference layer are left trainable\n",
    "for l in model.layers:\n",
    "    print(l, l.trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make saving directory for check point\n",
    "saveDir = \"./moe3Cifar10/\"\n",
    "if not os.path.isdir(saveDir):\n",
    "    os.makedirs(saveDir)\n",
    "\n",
    "# early stopping and model checkpoint\n",
    "es_cb = EarlyStopping(monitor=\"val_loss\", patience=2, verbose=1, mode=\"auto\")\n",
    "chkpt = os.path.join(saveDir, \"Cifar10_.{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "cp_cb = ModelCheckpoint(\n",
    "    filepath=chkpt, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"auto\"\n",
    ")\n",
    "\n",
    "# load the newest model data if exists\n",
    "model = getNewestModel(model, saveDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train0,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test0),\n",
    "    callbacks=[es_cb, cp_cb],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "mixture_loss_accuracy = model.evaluate(x_test, y_test0)\n",
    "print(mixture_loss_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "237325bcc29941e39fe72ee4eabe6e813d563223fa166689b5c641bed30c6686"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
